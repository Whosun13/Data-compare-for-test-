import streamlit as st
import pandas as pd
from io import BytesIO
from thefuzz import fuzz
from docx import Document
from docx.shared import Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from sentence_transformers import SentenceTransformer, util
import numpy as np
import textract  # .doc oâ€˜qish uchun

# ------------------ Modelni keshlash ------------------
@st.cache_resource
def load_model():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

model = load_model()

# ------------------ Lugâ€˜atlar ------------------
texts = {
    "uz": {
        "title": "ðŸ“Š Ma'lumotlarni Taqqoslash Platformasi",
        "upload_db": "1ï¸âƒ£ Ma'lumotlar bazasini yuklang (.xlsx, .csv, .doc, .docx, .txt)",
        "upload_check": "2ï¸âƒ£ Tekshiriladigan ma'lumotlarni yuklang yoki kiriting",
        "input_method": "Kiritish usuli",
        "file_upload": "Fayl yuklash",
        "manual_input": "Qo'lda kiritish",
        "load_db": "Bazani yuklash",
        "load_check": "Tekshiriladigan ma'lumotlar",
        "input_area": "Ma'lumotlarni kiriting (vergul yoki yangi qatordan ajratib)",
        "db_loaded": "**Yuklangan ma'lumotlar bazasi:**",
        "input_loaded": "**Tekshiriladigan ma'lumotlar:**",
        "select_column_db": "Bazadagi taqqoslanadigan ustunni tanlang",
        "select_column_input": "Tekshiriladigan fayldagi ustunni tanlang",
        "extra_columns": "Natijada ko'rsatish uchun qo'shimcha ustunlar",
        "similarity_slider": "Shakl o'xshashlik foizini tanlang (%)",
        "semantic_slider": "Ma'no o'xshashlik foizini tanlang (%)",
        "compare_btn": "Taqqoslash",
        "results": "Natijalar",
        "download_csv": "ðŸ“¥ Natijani yuklab olish (.csv)",
        "download_xlsx": "ðŸ“¥ Natijani yuklab olish (.xlsx)",
        "download_docx": "ðŸ“¥ Natijani yuklab olish (.docx)",
        "unsupported_format": "Qo'llab-quvvatlanmaydigan format"
    },
    "ru": {
        "title": "ðŸ“Š ÐŸÐ»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð° ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…",
        "upload_db": "1ï¸âƒ£ Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð±Ð°Ð·Ñƒ Ð´Ð°Ð½Ð½Ñ‹Ñ… (.xlsx, .csv, .doc, .docx, .txt)",
        "upload_check": "2ï¸âƒ£ Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð¸Ð»Ð¸ Ð²Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ",
        "input_method": "Ð¡Ð¿Ð¾ÑÐ¾Ð± Ð²Ð²Ð¾Ð´Ð°",
        "file_upload": "Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ñ„Ð°Ð¹Ð»",
        "manual_input": "Ð’Ð²ÐµÑÑ‚Ð¸ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ",
        "load_db": "Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð±Ð°Ð·Ñƒ",
        "load_check": "ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ",
        "input_area": "Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ñ‡ÐµÑ€ÐµÐ· Ð·Ð°Ð¿ÑÑ‚ÑƒÑŽ Ð¸Ð»Ð¸ Ð½Ð¾Ð²ÑƒÑŽ ÑÑ‚Ñ€Ð¾ÐºÑƒ)",
        "db_loaded": "**Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ð°Ñ Ð±Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…:**",
        "input_loaded": "**ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ:**",
        "select_column_db": "Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ ÑÑ‚Ð¾Ð»Ð±ÐµÑ† Ð´Ð»Ñ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ñ Ð² Ð±Ð°Ð·Ðµ",
        "select_column_input": "Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ ÑÑ‚Ð¾Ð»Ð±ÐµÑ† Ð²Ð¾ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…",
        "extra_columns": "Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑÑ‚Ð¾Ð»Ð±Ñ†Ñ‹ Ð´Ð»Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ðµ",
        "similarity_slider": "Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð° Ð¿Ð¾ Ñ„Ð¾Ñ€Ð¼Ðµ (%)",
        "semantic_slider": "Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð° Ð¿Ð¾ ÑÐ¼Ñ‹ÑÐ»Ñƒ (%)",
        "compare_btn": "Ð¡Ñ€Ð°Ð²Ð½Ð¸Ñ‚ÑŒ",
        "results": "Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹",
        "download_csv": "ðŸ“¥ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ (.csv)",
        "download_xlsx": "ðŸ“¥ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ (.xlsx)",
        "download_docx": "ðŸ“¥ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ (.docx)",
        "unsupported_format": "ÐÐµÐ¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼Ñ‹Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚"
    }
}

# ------------------ Matn normalizatsiyasi ------------------
def normalize_text(s):
    if pd.isna(s):
        return ""
    s = str(s).lower()
    apostrophes = ["â€™", "â€˜", "`", "Ê»", "â€›", "Â´", "ËŠ", "Ê½", "Ê¾", "Ê¿"]
    for apos in apostrophes:
        s = s.replace(apos, "'")
    return " ".join(s.split())

# ------------------ DOC/DOCX oâ€˜qish ------------------
def read_doc_or_docx(file):
    if file.name.endswith(".docx"):
        file_bytes = file.read()
        file.seek(0)
        doc = Document(BytesIO(file_bytes))
        if doc.tables:
            tables_data = []
            for table in doc.tables:
                for row in table.rows:
                    row_data = [cell.text.strip() for cell in row.cells]
                    tables_data.append(row_data)
            df = pd.DataFrame(tables_data)
            df.columns = df.iloc[0]
            df = df[1:].reset_index(drop=True)
            return df
        full_text = [para.text.strip() for para in doc.paragraphs if para.text.strip()]
        return pd.DataFrame(full_text, columns=["Data"])
    else:  # .doc fayl
        text = textract.process(file.name).decode("utf-8", errors="ignore")
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        return pd.DataFrame(lines, columns=["Data"])

# ------------------ Fayl yuklash ------------------
def load_file(file):
    if file.name.endswith(".xlsx"):
        return pd.read_excel(file)
    elif file.name.endswith(".csv"):
        return pd.read_csv(file)
    elif file.name.endswith(".doc") or file.name.endswith(".docx"):
        return read_doc_or_docx(file)
    elif file.name.endswith(".txt"):
        text = file.read().decode("utf-8", errors="ignore")
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        return pd.DataFrame(lines, columns=["Data"])
    else:
        st.error(current_texts["unsupported_format"])
        return None

# ------------------ Word saqlash ------------------
def df_to_word(df):
    doc = Document()
    doc.add_heading(current_texts["results"], level=1)
    table = doc.add_table(rows=1, cols=len(df.columns))
    table.style = 'Table Grid'
    hdr_cells = table.rows[0].cells
    for i, col_name in enumerate(df.columns):
        hdr_cells[i].text = str(col_name)
        para = hdr_cells[i].paragraphs[0]
        para.alignment = WD_ALIGN_PARAGRAPH.CENTER
        run = para.runs[0]
        run.font.bold = True
        run.font.size = Pt(11)
    for _, row in df.iterrows():
        row_cells = table.add_row().cells
        for i, val in enumerate(row):
            row_cells[i].text = str(val)
    f = BytesIO()
    doc.save(f)
    f.seek(0)
    return f

# ------------------ Interfeys ------------------
lang = st.selectbox("Til / Ð¯Ð·Ñ‹Ðº", options=["O'zbekcha", "Ð ÑƒÑÑÐºÐ¸Ð¹"])
current_texts = texts["uz"] if lang == "O'zbekcha" else texts["ru"]

st.title(current_texts["title"])
st.subheader(current_texts["upload_db"])
uploaded_db = st.file_uploader(current_texts["load_db"], type=["xlsx", "csv", "doc", "docx", "txt"])

st.subheader(current_texts["upload_check"])
input_type = st.radio(current_texts["input_method"], [current_texts["file_upload"], current_texts["manual_input"]])

input_data = None
if input_type == current_texts["file_upload"]:
    uploaded_check = st.file_uploader(current_texts["load_check"], type=["xlsx", "csv", "doc", "docx", "txt"])
    if uploaded_check is not None:
        input_data = load_file(uploaded_check)
elif input_type == current_texts["manual_input"]:
    raw_text = st.text_area(current_texts["input_area"])
    if raw_text.strip():
        items = [x.strip() for x in raw_text.replace("\n", ",").split(",") if x.strip()]
        input_data = pd.DataFrame(items, columns=["InputData"])

if uploaded_db is not None:
    df = load_file(uploaded_db)
    if df is not None:
        st.write(current_texts["db_loaded"])
        st.dataframe(df)

        if input_data is not None:
            st.write(current_texts["input_loaded"])
            st.dataframe(input_data)

            column_to_check = st.selectbox(current_texts["select_column_db"], df.columns)
            input_column_to_check = st.selectbox(current_texts["select_column_input"], input_data.columns)
            extra_columns = st.multiselect(current_texts["extra_columns"], [col for col in df.columns if col != column_to_check])

            similarity_threshold = st.slider(current_texts["similarity_slider"], 50, 100, 80, 1)
            semantic_threshold = st.slider(current_texts["semantic_slider"], 50, 100, 80, 1)

if st.button(current_texts["compare_btn"]):
    df["__norm_col__"] = df[column_to_check].apply(normalize_text)
    input_data["__norm_input__"] = input_data[input_column_to_check].apply(normalize_text)

    db_sentences = df[column_to_check].astype(str).unique().tolist()
    input_sentences = input_data[input_column_to_check].astype(str).tolist()

    db_embeddings = model.encode(db_sentences, convert_to_tensor=True)
    input_embeddings = model.encode(input_sentences, convert_to_tensor=True)

    filtered_input = input_data[input_data["__norm_input__"].astype(bool)]

    results = []
    for idx, (original, item) in enumerate(zip(filtered_input[input_column_to_check], filtered_input["__norm_input__"])):
        match_rows = df[df["__norm_col__"] == item]
        exact_match = not match_rows.empty

        similar_items = [val for val in df["__norm_col__"].unique()
                         if fuzz.ratio(item, val) >= similarity_threshold and val != item]

        semantic_similar_items = []
        cos_scores = util.pytorch_cos_sim(input_embeddings[idx], db_embeddings)[0]
        for i, score in enumerate(cos_scores):
            if score >= (semantic_threshold / 100.0) and db_sentences[i] != original:
                semantic_similar_items.append(db_sentences[i])

        extra_data = {}
        for col in extra_columns:
            extra_data[col] = ", ".join(match_rows[col].astype(str).unique()) if exact_match else ""

        results.append({
            "Kiritilgan": original,
            "Mavjud": "Ha" if exact_match else "Yo'q",
            "O'xshashlar": ", ".join(similar_items) if similar_items else "-",
            "Semantik o'xshashlar": ", ".join(set(semantic_similar_items)) if semantic_similar_items else "-",
            **extra_data
        })

    result_df = pd.DataFrame(results)
    st.subheader(current_texts["results"])
    st.dataframe(result_df)

    csv = result_df.to_csv(index=False).encode('utf-8')
    st.download_button(current_texts["download_csv"], csv, "natijalar.csv", "text/csv")

    towrite = BytesIO()
    result_df.to_excel(towrite, index=False, engine='openpyxl')
    towrite.seek(0)
    st.download_button(current_texts["download_xlsx"], towrite, "natijalar.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

    word_file = df_to_word(result_df)
    st.download_button(current_texts["download_docx"], word_file, "natijalar.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document")
